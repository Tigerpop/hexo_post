---
layout: posts
title: transform模型
date: 2025-10-22 18:59:50
description: "这是文章开头，显示在主页面，详情请点击此处。"
categories: 
- "机器学习"
tags:
- "注意力机制"
- "注意力矩阵"
- "transform"
---



# 简介

![截屏2025-10-21 21.33.27](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.33.27.jpg)



![截屏2025-10-21 21.35.19](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.35.19.jpg)

![截屏2025-10-21 21.36.03](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.36.03.jpg)

![截屏2025-10-21 21.38.25](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.38.25.jpg)

![截屏2025-10-21 21.40.27](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.40.27.jpg)

![截屏2025-10-21 21.41.32](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.41.32.jpg)



# 自注意力机制

![截屏2025-10-21 21.45.37](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.45.37.jpg)

![截屏2025-10-22 15.02.22](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2015.02.22-1116648.png)

![截屏2025-10-22 15.23.36](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2015.23.36.png)

- 需要提一下，QKV三者的维度是一样的。dk 是key 的维度也是其他三者的维度，比如512维。

- 计算相似度的方法有很多，不一定都是用点积，有高斯距离等等其他的计算相似度的方法都是可以选择的。点积的话结果越大，相似度越高。

- softmax 算出来的 都是 0-1 之间的值，来把前面的结果变成概率。

- 如果直接用点积结果进入softmax 可能导致概率极大和概率极小，导致梯度爆炸或者消失，所以点积结果除一下根号的维度，来让最后softmax输出的数据变柔和一些。

  

![截屏2025-10-21 21.46.55](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.46.55.jpg)

![截屏2025-10-21 21.48.07](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.48.07.jpg)

![截屏2025-10-21 21.49.33](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.49.33.jpg)

![截屏2025-10-21 21.50.09](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.50.09.jpg)

![截屏2025-10-21 21.50.34](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-21%2021.50.34.jpg)

![截屏2025-10-22 09.15.42](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.15.42.jpg)

![截屏2025-10-22 09.16.07](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.16.07.jpg)

![截屏2025-10-22 09.16.23](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.16.23.jpg)

![截屏2025-10-22 09.16.40](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.16.40.jpg)

![截屏2025-10-22 09.16.59](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.16.59.jpg)

![截屏2025-10-22 09.17.14](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.17.14.jpg)

![截屏2025-10-22 09.18.02](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.18.02.jpg)

![截屏2025-10-22 09.18.16](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.18.16.jpg)

![截屏2025-10-22 09.18.32](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.18.32.jpg)

## 总结

Q、K、V 来自同一组输入的时候，就是自注意力。它相当于注意力机制中的一个特例。

小明看小红长相就是注意力机制，小红自己看自己就是自注意力机制。![截屏2025-10-22 17.26.55](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2017.26.55.png)

注意上面的Attention部分，只有左边的是自注意力机制，右边上面是交叉注意力机制，右边下面是掩码注意力机制，

- 所谓 “掩码注意力机制” 用一个例子说明：【我吃零食】是预测目标，在预测时不能把目标全部公开，相当于需要闭卷答题，【我 X X X】掩住后面要预测的部分。
- “交叉注意力机制”中，Q 来自于 Decoder 的输入，K、V 来自 Encoder 的输出。【它就不是自注意力机制，因为它有别的地方的输入。】



# 多头注意力机制

![截屏2025-10-22 17.45.55](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2017.45.55.png)

![截屏2025-10-22 17.47.04](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2017.47.04.png)

![截屏2025-10-22 17.48.22](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2017.48.22.png)

![截屏2025-10-22 18.26.09](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2018.26.09.png)

![截屏2025-10-22 18.31.26](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2018.31.26.png)



![截屏2025-10-22 09.18.48](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.18.48.jpg)

![截屏2025-10-22 09.19.02](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2009.19.02.jpg)



# 基础演示

![截屏2025-10-22 10.12.34](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.12.34.jpg)

![截屏2025-10-22 10.18.25](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.18.25.jpg)

![截屏2025-10-22 10.19.05](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.19.05.jpg)

![截屏2025-10-22 10.19.40](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.19.40.jpg)

![截屏2025-10-22 10.19.53](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.19.53.jpg)

![截屏2025-10-22 10.20.42](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.20.42.jpg)

![截屏2025-10-22 10.21.17](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.21.17.jpg)

![截屏2025-10-22 10.21.33](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.21.33.jpg)

![截屏2025-10-22 10.22.03](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.22.03.jpg)

# 并行计算

![截屏2025-10-22 10.22.36](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.22.36.jpg)

![截屏2025-10-22 10.22.48](transform%E6%A8%A1%E5%9E%8B/%E6%88%AA%E5%B1%8F2025-10-22%2010.22.48.jpg)