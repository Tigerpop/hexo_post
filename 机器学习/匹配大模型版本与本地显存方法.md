---
layout: posts
title: 匹配大模型版本与本地显存方法
date: 2025-9-7 22:05:02
description: "这是文章开头，显示在主页面，详情请点击此处。"
categories: 
- "机器学习"
tags:
- "显存"


---



我们看见大量的大语言模型在 modelscope 这类平台上，如何确定我们本地的GPU显存能否匹配上这个模型的基本要求？

这次我们搞这个。 

# 看模型显存需求

大模型的计算单位都是2字节，也就是8*2=16bit ；而 我们看见 标着 AWQ 或者 4int 等就是量化压缩到4bit 的意思，最终显存需求会是原来的1/4。

Qwen-32B-AWQ 

指的是32B个计算单位，原本FP16单位是32*2=64G的显存需求，但是AWQ的压缩，导致只需要64/4= 16G显存。

Qwen2.5-72B-AWQ

指的是72B个计算单位，原本FP16 是72*2=144G的显存需求，但是AWQ的压缩，导致只需要144/4= 36G显存。

通义千问 QwQ-32B

指的是32B个计算单位，FP16 大小就是 16bit的单位，也就是32*2=64G的显存需求。



# 计算任务拆分

ray 搞一个2个机器的集群，然后vllm 来平分GPU显存去计算。核心是vllm对计算的拆分。 

例如：

`tensor_parallel_size=4` ，我们把计算任务用vllm 分4份给到GPU集群。 

我们的算力集群是由4张24G显存的GPU组成。 



# GPU单卡显存计算公式

## 总需求

`单卡需求(GB) = (量化模型大小 ÷ tensor_parallel_size) + KV_Cache + 开销`

## 模型分片

模型分片 = (量化模型大小 ÷ tensor_parallel_size)

## KV_Cache

`KV_Cache ≈ 2 × batch_size × seq_len × 层数 × hidden_size × dtype_size × 0.6`

大约`KV_Cache(GB) ≈ batch_size × seq_len × 0.0001`

下方为 KV_Cache(GB) 例表：

| batch=4, seq=2048  | ~0.8 GB  |
| ------------------ | -------- |
| batch=8, seq=4096  | ~3.2 GB  |
| batch=16, seq=8192 | ~12.8 GB |

| **vLLM** | `--max-num-seqs`           | 最大并发请求数（直接影响 batch_size） |
| -------- | -------------------------- | ------------------------------------- |
|          | `--max-num-batched-tokens` | 批处理的最大 token 数                 |

| **vLLM** | `--max-model-len`          | 模型支持的最大序列长度（直接影响  seq_len） |
| -------- | -------------------------- | ------------------------------------------- |
|          | `--max-num-batched-tokens` | 间接限制 seq_len                            |

## 开销

开销（固定值）

- 中间激活值：1-2 GB

- 框架开销：1-2 GB

- **总计**：2-4 GB

  

## 启动峰值显存

启动峰值显存比稳态高 15-30%



# 查看GPU单卡显存压力

例如：

Qwen/Qwen-32B-Chat

**FP16 大小**：32B × 2 = 64 GB

KV_Cache(GB) ≈ batch_size × seq_len × 0.0001 =  8 × 8192 × 0.0001 = 6.5536 GB ≈ 6.4 GB 

| `batch_size` | 8    | 并发请求数 |
| ------------ | ---- | ---------- |
| `seq_len`    | 8192 | 上下文长度 |

**单卡需求**：

- 模型分片：64GB ÷ 4 = 16 GB
- KV Cache：6.4 GB
- 开销：3.0 GB

单卡需求 =  25.4 > 我单一GPU 24G显存

所以我无法部署此大模型。 

同时，由于启动峰值显存比稳态高 15-25%，上面的vllm配置中，即便是batch_size=1,seq_len=512, 在峰值依然超过了 显存余量，所以我无法部署此大模型。 



例如：

通义千问2.5-72B-Instruct-AWQ量化

**单卡需求**：

- 模型分片：36GB ÷ 4 = 9 GB
- KV Cache：6.4 GB
- 开销：3.0 GB

单卡需求 =  18.4 > 我单一GPU 24G显存

启动峰值显存 ≈  18.4 * 1.25 = 23 < 我单一GPU 24G显存, 所以可以部署。



# 查看GPU总显存压力

满足一下即可：

模型需要显存大小 < vllm总显存 

 